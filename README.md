# Classifica√ß√£o de Quest√µes de Vestibular

Modelo de Machine Learning para Classifica√ß√£o de Quest√µes de Vestibular

## I - Exposi√ß√£o do Problema üöÄ

Este projeto iniciou-se a partir de um problema da startup em que fa√ßo parte. Um certo momento, levantou-se a necessidade de fornecer aos nossos usu√°rios, quest√µes de vestibulares para que eles possam estudar mais e ter mais conte√∫dos para se darem bem no vestibular.
Assim, criou-se um banco de dados com 120 mil quest√µes
Por√©m, ap√≥s validarmos os dados, identificou-se que nem todos as quest√µes estavam classificadas de acordo com o assunto que elas pertenciam.
Assim, surgiu a ideia de implementar um modelo e trein√°-lo, a fim de classificar cada quest√£o de acordo com o assunto que essa quest√£o pertence.

### Modelo
Para isso, foi necess√°rio utilizar alguma t√©cnica para a avalia√ß√£o de textos.
Assim, idealizou-se usar o bag of words. Uma explica√ß√£o simples √© que o bag of words √© uma lista que contem todas as palavras que est√£o nos textos de maneira n√£o repetida.
Utilizamos ela para poder identificar as palavras mais recorrentes e entender se ela agregam na classifica√ß√£o das quest√µes.

## II - Importa√ß√£o dos dados

Como os dados estavam no bando do Mongo. utilizei o framework do MongoDB para conectar e importar os dados para o Python

```
client = MongoClient(config["MONGO_CONNECTION_STRING"])
database = client["revisapp"]
collection = database["questions"]
```


## III - Prepara√ß√£o dos dados
Para a aquisi√ß√£o das quest√µes, foi feito uma ferramenta de webscrapping para captura das quest√µes.
Assim, os dados s√£o recebidos e inseridos no banco no formato HTML
Com isso, foi necess√°rio convert√™-los em texto utilizado a biblioteca BeautifulSoup.

```
from bs4 import BeautifulSoup

for i in range(len(corpus )):
    corpus[i] = corpus[i].lower()
    corpus[i] = BeautifulSoup(corpus [i]).get_text() # transforma o HTML em texto
    corpus [i] = re.sub(r'\W',' ',corpus [i])  # remove os caracteres especiais
    corpus [i] = re.sub(r'\s+',' ',corpus [i]) # remove os caracteres especiais
    
```

Uma vez que os dados foram limpos e com seus caracteres especias removidos, foi necess√°rio fazer o processo de _Stemming_, que consiste em reduzir palavras relacionadas a uma forma m√≠nima comum, de forma que possam ser combinadas sob uma √∫nica representa√ß√£o, chamada de _stem_. Com este processo, obt√©m-se √≠ndices mais enxutos, melhorando assim a qualidade dos dados para o modelo.

Assim, foi criado uma fun√ß√£o com esse objetivo, utilizando a biblioteca _nltk.stem_.

```
from nltk.tokenize import word_tokenize
from nltk.stem import RSLPStemmer
from unidecode import unidecode

stemmer = RSLPStemmer()

def stemSentence(sentence):
    token_words = word_tokenize(sentence)
    token_words
    stem_sentence = []
    for word in token_words:
        if word not in nltk.corpus.stopwords.words('portuguese'):
            word = stemmer.stem(word)
            word = unidecode(word)
            stem_sentence.append(word)  
            stem_sentence.append(" ")
    return "".join(stem_sentence)
```

Essa fun√ß√£o criada, al√©m de fazer o _stem_, tamb√©m remove os acentos e os _stopwords_. 
Os _stopwords_ s√£o palavras que podem ser consideradas irrelevantes para o conjunto de resultados a ser exibido em uma busca realizada em uma search engine. Exemplos: as, e, os, de, para, com, sem, foi, etc..

Assim, as palavras a serem consideradas s√£o reduzidas, conforme exemplo abaixo::

```
'med centimetr lad triangul express x 1 2x x2 5 progress aritme ness ord calcul perimetr triangul ',
'sequ figur desenh malh quadricul indic tre prim etap form fract cad quadr dess malh are 1 cm2 ',
'consider med metr lad triangul progress geometr ness ord express x 1 2x x2 corret afirm perimetr dess triangul med ',
'sequ infinit triangul equilater pod ser constru inscrev triangul dentr outr part prim ',
'consid triangul i ii iii caracter abaix atraves med lad triangul i 9 12 15 triangul ii 5 12 13 triangul iii 5 7 9 qual triangul retangul med lad progress aritme ',
'sequ x 1 2x 1 5x 3 constitu progress aritme tre term cuj val represent med centimetr lad triangul dess mod corret afirm perimetr dess triangul centimetr igual ',
```

Como parte do processo de processamento de texto, √© necess√°rio verificar quais palavras s√£o as mais frequentes, para que sejam usadas posteriormente.
Assim, foi utilizado a fun√ß√£o "word_tokenize", que pega uma senten√ßa e separa os dados em posi√ß√µes de uma lista.

```
wordfreq = {}
for sentence in corpus:
    tokens = nltk.word_tokenize(sentence)
    for token in tokens:
        if token not in wordfreq.keys():
            wordfreq[token] = 1
        else:
            wordfreq[token] += 1
```

Agora, ordena-se essa lista considerando aqueles termos que aparecem com mais frequ√™ncia. No nosso caso, selecionamos as 150 palavras mais frequ√™ntes.

```
import heapq

MOST_FREQUENT_NUMBER = 150

most_freq = heapq.nlargest(MOST_FREQUENT_NUMBER, wordfreq, key=wordfreq.get)
```

O ultimo passo √© criar o saco de palavras transcrevendo cada documento para uma informa√ß√£o booleana dizendo se cada palavra do saco de palavras est√° presente ou n√£o no documento. Se a palavra estiver na senten√ßa, coloca 1, se n√£o, 0.

```
sentence_vectors = []
for sentence in corpus:
    sentence_tokens = nltk.word_tokenize(sentence)

    sent_vec = []
    for token in most_freq:
        if token in sentence_tokens:
            sent_vec.append(1)
        else:
            sent_vec.append(0)
    sentence_vectors.append(sent_vec)

sentence_vectors = np.array(sentence_vectors)
```

Uma vez que o modelo foi desenhado para classificar os assuntos de matem√°tica, foi necess√°rio separar essa √∫nica classifica√ß√£o "conjuntas" em diversas classifica√ß√µes. Assim, levantou-se todas as possibilidades de assuntos existentes no banco de quest√µes de matem√°tica, criando assim um _array_ com 81 assuntos distintos.

```
subjects = ['√Ålgebra', 'Probabilidade e estat√≠stica', 'N√∫meros', 'No√ß√µes de l√≥gica',
            'No√ß√µes de L√≥gica Matem√°tica', 'Matem√°tica Financeira', 'Grandezas e medidas', 'Raz√£o e Propor√ß√£o',
            'Geometria', 'Estat√≠stica', 'Determinantes', '√Ålgebra linear', 'An√°lise Combinat√≥ria', 'Arcos na Circunfer√™ncia',
            '√Årea e Per√≠metro das Figuras Planas', 'Aritm√©tica', 'Arranjo', 'C√°lculo diferencial integral', 'Cilindros', 'Circunfer√™ncia',
            'Circunfer√™ncia e C√≠rculo', 'Combina√ß√£o', 'Comprimento', 'Volume', 'Cones', 'Esfera', 'Congru√™ncia de Tri√¢ngulos',
            'C√¥nicas', 'Conjuntos', 'Conjuntos Fun√ß√£o', 'Decimal', 'Equa√ß√£o do Primeiro Grau', 'Equa√ß√£o do Segundo Grau',
            'Equa√ß√µes', 'Equa√ß√µes polinomiais', 'Equa√ß√µes polinomiais Exponenciais', 'Esfera', 'Express√µes alg√©bricas', 'Express√µes alg√©bricas',
            'Fatorial', 'Fun√ß√£o', 'Fun√ß√£o Exponencial', 'Fun√ß√£o Logar√≠tmica', 'Raz√µes e propor√ß√µes',
            'Fun√ß√£o Quadr√°tica', 'Fun√ß√£o Trigonometria', 'Fun√ß√µes Definidas por V√°rias Senten√ßas', 'Fun√ß√µes Trigonom√©tricas', 'Fundamentos',
            'Geometria anal√≠tica', 'Geometria espacial', 'Geometria plana', 'Gr√°ficos', 'Inequa√ß√£o do Segundo Grau', 'Inequa√ß√µes', 'Inequa√ß√µes polinomiais',
            'Inequa√ß√µes polinomiais', 'Juros Compostos', 'Juros Simples', 'L√≥gica matem√°tica', 'Prismas', 'M√©dias',
            'Ponderada', 'Porcentagem', 'Probabilidade', 'M√∫ltiplos e Divisores', 'Nota√ß√£o cient√≠fica',
            'Outros', 'Permuta√ß√£o', 'Pir√¢mides', 'Pol√≠gonos', 'Princ√≠pio Fundamental da Contagem', 'Prismas', 
            'Problemas sobre as 4 opera√ß√µes', 'Raz√µes Trigonom√©tricas no Tri√¢ngulo Ret√¢ngulo', 'Rela√ß√µes M√©tricas do Tri√¢ngulo Ret√¢ngulo',
            'Rela√ß√µes M√©tricas em Tri√¢ngulos Quaisquer', 'Reta', 'Retas e Planos', 'Sequ√™ncias', 'Sistema de Numera√ß√£o e M√©trico', 'Superf√≠cie Poli√©drica e Poliedros',
            'Tempo', 'Trigonometria', 'Troncos']
```

Como feito nas quest√µes, o _output_ foi dividido igual o bag of words implementado no enunciado das quest√µes. Para o treinamento do modelo, foi colocado 1 ou 0 se o assunto pertencia √†quela quest√£o ou n√£o.

```
df = documents
for index, document in df.iterrows():
    i = 0
    for subject in subjects:
        i += 1
        column_name = "tag_"+str(i)
        if (subject in document["level_2"]) or (subject in document["level_3"]):
            df.loc[index, subject] = 1
        else:
            df.loc[index, subject] = 0
```




## IV - An√°lise explorat√≥ria

A an√°lise explorat√≥ria foi interessante para entender os dados. 
Para isso, foi feito a distribui√ß√£o dos dados atrav√©s da plotagem de histogramas.

Assim, algumas quest√µes foram levantadas para que os dados nos respondessem, conforme a seguir.

### Quais s√£o os assuntos mais recorrentes?

Foi verificado os assuntos de matem√°tica mais recorrentes, para entender a distribui√ß√£o dos valores

![plot](./fig_subject_count.png)

### Quais s√£o as palavras mais recorrentes?

Foi validado quais eram as palavras mais recorrentes, para validar se n√£o haviam palavras que n√£o deveriam estar presentes.

![plot](./fig_word_count.png)


![plot](./fig_wordcloud.png)

## V - Modelagem

O modelo proposto foi desenhado de acordo com a classifica√ß√£o j√° existente no banco de dados. As classifica√ß√µes existentes consistiam na concatena√ß√£o dos assuntos em que a quest√£o estava envolvida.

Exemplos de classifica√ß√£o:
* √Ålgebra Matem√°tica Financeira N√∫meros
* No√ß√µes de l√≥gica No√ß√µes de L√≥gica Matem√°tica
* √Ålgebra Grandezas e medidas Raz√£o e Propor√ß√£o

Pode-se observar que h√° diversos assuntos para uma quest√£o √∫nica, o que nos mostra que esse √© um problema com multiplas sa√≠das/resultados.


### testes de classificadores

Como o problema consiste em uma classifica√ß√£o de multiplas sa√≠das, foi utilizado a classe _MultiOutputClassifier_ da biblioteca _sklearn_.
Para que essa classe funcione corretamente, √© necess√°rio escolher um _estimator_, que, nesse caso, utilizamos o _RandomForestClassifier_.

O nome (Classificador Floresta Aleat√≥ria) explica muito bem o funcionamento do algoritmo, que ir√° criar muitas √°rvores de decis√£o, de maneira aleat√≥ria, formando o que podemos enxergar como uma floresta, onde cada √°rvore ser√° utilizada na escolha do resultado final.

Durante o processo de contru√ß√£o do modelo, tambpem foi testado o _KNeighborsClassifier_ e o _MLPClassifier_, por√©m com resultados inferiores ao _RandomForestClassifier_.

```
clf = MultiOutputClassifier(RandomForestClassifier(max_depth=24, min_samples_leaf=6))
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
```

Agora, √© feito o teste para avaliar se o treinamento deu certo.
Crio-se um vetor (hits) que me mostra quais predi√ß√µes foram corretas. Como se trata de um problema de multiplas sa√≠das, foi avaliado se pelo menos 1 assunto foi predito corretamente.
No in√≠cio, foi testado apenas as classifica√ß√µes que bateram 100% com as classifica√ß√µes de teste, por√©m, os valores eram muito baixos e n√£o fez sentido ao problema que t√≠nhamos.
Nesse caso e aplicado ao problema que t√≠nhamos, tendo apenas uma ou mais classifica√ß√µes corretas, ja nos atendia e resolvia o problema.

```
hits = np.any((y_pred + y_test) > 1, axis=1)
print("taxa de acerto: ", round(hits.sum()/len(hits)*100,2), "%")
```

Assim, ap√≥s in√∫meros treinos e respostas do modelo, observou-se que temos uma taxa de acerto m√©dia de 65%.

![plot](./fig_model_output.png)


## VI - Conclus√£o 

A implementa√ß√£o do modelo foi desafiador. A falta de vis√£o matem√°tica dos dados atrapalhou um pouco o desenvolvimento do modelo. A mentoria realizada pela equipe de profissionais da awari foi fundamental em todo o processo de desenvolvimento, monstrando onde estavam os gaps do meu conhecimento e onde eu poderia melhorar para chegar no resultado final.

Da implementa√ß√£o, foi extra√≠do um modelo que ser√° utilizado no aplicativo RevisApp. A id√©ia √© utilizar o modelo para otimizar os estudos dos usu√°rios do _app_ atrav√©s de uma funcionalidade ainda em implementa√ß√£o. Com isso, poderemos indicar aos nossos usu√°rios quais assuntos ele tem mais dificuldade e, consequentemente, quais precisam ser estudados com mais intensidade, sugerindo assim o conte√∫ido e quest√µes similares dos assuntos em d√©ficit de conhecimento.





######################
    COLOCAR OQ UE FOI EXTRA√çDO
    CONCLUS√ïES
    DISCUSS√ïES
    LI√áOES APRENDIDAS
    ONDE VOU UTILIZAR ESSA AN√ÅLISE/MODELO
        - quest√µes n√£o classificadas do RevisApp
        - Solu√ß√£o mais completa para os usu√°rios
        - Desafio do Dia
        - Direcionamento para o usu√°rio estudar os assuntos que tem mais "car√™ncia"
######################

########################################################################
‚óè Divis√£o dos dados em dados de treino e teste
‚óè Cria√ß√£o de um benchmark (modelo inicial para compara√ß√µes futuras)
‚óè Triagem de modelo(s) para utiliza√ß√£o
‚óè Utiliza√ß√£o de m√©tricas de mensura√ß√£o de performance dos algoritmos
‚óè Calibra√ß√£o dos hiperpar√¢metros do(s) algoritmo(s)

########################################################################
Anota√ß√µes
########################################################################
classifica√ß√£o

agrupar quest√µes

bag of words
n grams
########################################################################

Seguem as a√ß√µes que conversamos agora:

1. elaborar a √°rvore com disciplinas e assuntos que ser√£o mapeados na classifica√ß√£o;

2. preparar a base de dados com quest√µes previamente classificadas dentro segundo os assuntos mapeados na √°rvore;

3. estudar as primeiras t√©cnicas para classifica√ß√£o de texto como bag-of-words e regress√£o log√≠stica;

4. estudar t√©cnicas mais avan√ßadas para classifica√ß√£o de texto.

--- proposi√ß√µes

########################################################################
removo stopwords

procurar biblioteca com stopwords

sigmoide logistica

cross validation:
separa em 2 partes - uma parte pra treinar - outra pra testar
matriz de confus√£o.
########################################################################

T√≥picos da mentoria 05/06

Converter HTML para texto (ex.: UTF-8, ASCII)

Uso da classe CountVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html

Stopwords: https://gist.github.com/alopes/5358189

Valida√ß√£o cruzada:

- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html

Matriz de confus√£o: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html

Atividades posteriores:

- valida√ß√£o cruzada (KFold estratificado): https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html

- bag of words normalizado (TfidfVectorizer): https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html

- reposit√≥rio com c√≥digos de exemplo: https://github.com/alex-carneiro/Moving2DS

############################################################################
26/06/2021
Clustering: https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68

Clustering com Scikit-Learn: https://scikit-learn.org/stable/modules/clustering.html

Overfit: https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/

Dist√¢ncia de Levenshtein: http://people.cs.pitt.edu/~kirk/cs1501/Pruhs/Spring2006/assignments/editdistance/Levenshtein%20Distance.htm

Dist√¢ncia de Levenshtein com Python: https://pypi.org/project/python-Levenshtein/

Uso de vota√ß√£o para associar novas tags para as senten√ßas ap√≥s clusteriza√ß√£o

TODO: elaborar o pipeline sequencial que descreva as tarefas para agrupamento e mapeamento das quest√µes

V√≠deo sobre descri√ß√£o autom√°tica de cenas: https://www.youtube.com/watch?v=40riCqvRoMs

############################################################################
10/07/2021
Como definir o n√∫mero ideal de clusters para o K Means: https://jtemporal.com/kmeans-and-elbow-method/



########################################
## BAG OF WORDS
### Observa√ß√µes importantes
O saco de palavras permite que voc√™ utilize classificadores e fa√ßa outras an√°lises posteriormente. Criar um saco de palavra n√£o te d√° informa√ß√£o alguma instantaneamente.   O saco de palavras utilizando a incid√™ncia das palavras pode ser utilizado, por√©m, esse modelo possui problemas j√° bem conhecidos. S√£o eles: (1) ‚Äúperda‚Äù de informa√ß√£o sint√°tica, considerando que se trata de uma abordagem estat√≠stica. (2) Modelos que consideram a frequ√™ncia inversa de palavras no conjunto de documentos j√° provaram ser mais eficientes em muitos casos.

https://www.computersciencemaster.com.br/como-criar-um-saco-de-palavras-em-python/
########################################

---
‚å®Ô∏è por [Mauricio Freitas](https://github.com/msfreit)